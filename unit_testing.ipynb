{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb13067e",
   "metadata": {},
   "source": [
    "\n",
    "# CS 6601: Artificial Intelligence - Assignment 4 - Decision Trees and Forests\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "Clone this repository:\n",
    "\n",
    "`git clone https://github.gatech.edu/omscs6601/assignment_4.git`\n",
    "\n",
    "\n",
    "You will be able to use numpy, math, time and collections.Counter for the assignment\n",
    "\n",
    "You will be able to use sklearn and graphviz for jupyter notebook visualization only\n",
    "\n",
    "No other external libraries are allowed for solving this problem.\n",
    "\n",
    "Please use the ai_env environment from previous assignments\n",
    "\n",
    "```\n",
    "conda activate ai_env\n",
    "```\n",
    "\n",
    "The supplementary testing notebooks use jupyter: visualize_tree and unit_testing. From your ai_env Terminal:\n",
    "\n",
    "```\n",
    "pip install graphviz==0.17.0\n",
    "or alternatively\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "If you have difficulty or errors on graphviz0.17 From your ai_env Terminal:\n",
    "```\n",
    "conda install -c conda-forge python-graphviz\n",
    "```\n",
    "which installs version 0.19 (compatible).\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "Machine learning offers a number of methods for classifying data into discrete categories, such as k-means clustering. Decision trees provide a structure for such categorization, based on a series of decisions that led to separate distinct outcomes. In this assignment, you will work with decision trees to perform binary classification according to some decision boundary. Your challenge is to build and to train decision trees capable of solving useful classification problems. You will learn first how to build decision trees, then how to effectively train them and finally how to test their performance.\n",
    "\n",
    "<p>\n",
    "<img src=\"./files/dt.png\" alt=\"Decision Trees\" width=\"700\" height=\"350\"/>\n",
    "\n",
    "\n",
    "## Submission and Due Date\n",
    "\n",
    "The deliverable for the assignment is a **_submission.py_** upload to Gradescope.\n",
    "\n",
    "* All functions to be completed in **_submission.py_**\n",
    "\n",
    "**Important**:\n",
    "Submissions to Gradescope are rate limited for this assignment. **You can submit two submissions every 60 minutes during the duration of the assignment**.\n",
    "\n",
    "In your Gradescope submission history, you can mark a certain submission as 'Active'. Please ensure this is your best submission.\n",
    "\n",
    "### The Files\n",
    "\n",
    "You will only have to edit and submit **_submission.py_**, but there are a number of notable other files:\n",
    "1. **_submission.py_**: Where you will build your decision tree, confusion matrix, performance metrics, forests, and do the vectorization warm up.\n",
    "2. **_decision_trees_submission_tests.py_**: Sample tests to validate your trees, learning, and vectorization locally.\n",
    "3. **_visualize_tree.ipnb_**: Helper Notebook to help you understand decision trees of various sizes and complexity\n",
    "4. **_unit_testing.ipynb_**: Helper Notebook to run through tests sequentially along with the readme\n",
    "\n",
    "### Resources\n",
    "1. **_Udacity Videos_**: [Lecture 7 on Machine Learning](https://classroom.udacity.com/courses/ud954/lessons/6808838653/concepts/67917548570923)  \n",
    "2. **_Textbook:Artificial Intelligence Modern Approach_**\n",
    "    Chapter 18 Learning from Examples\n",
    "    Chapter 20 Learning Probabilistic Models\n",
    "3. **_Cross-validation_** (https://en.wikipedia.org/wiki/Cross-validation_(statistics))\n",
    "4. **_K-Fold Cross-validation_** (https://tibshirani.su.domains/sta306bfiles/cvwrong.pdf)\n",
    "\n",
    "### Decision Tree Datasets\n",
    "     \n",
    "1. **_hand_binary.csv_**: 4 features, 8 examples, binary classification (last column)\n",
    "2. **_hand_multi.csv_**: 4 features, 12 examples, 3 classes, multi-class classification (last column)\n",
    "3. **_simple_binary.csv_**: 5 features, 100 examples, binary classification (last column)\n",
    "4. **_simple_multi.csv_**: 6 features, 100 examples, 3 classes, multi-class classification (last column)\n",
    "5. **_mod_complex_binary.csv_**: 7 features, 1400 examples, binary classification (last column)\n",
    "6. **_mod_complex_multi.csv_**: 10 features, 1800 examples, 5 classes, multi-class classification (last column)\n",
    "7. **_complex_binary.csv_**: 10 features, 5400 examples, binary classification (last column)\n",
    "8. **_complex_multi.csv_**: 16 features, 10800 examples, 9 classes, multi-class classification (last column)\n",
    "\n",
    "#### Not provided, but will have less class separation and more centroids per class. Complex sets given for development\n",
    "     - challenge_binary.csv: 10 features, 5400 examples, binary classification (last column)\n",
    "     - challenge_multi.csv: 16 features, 10800 examples, 9 classes, multi-class classification (last column)\n",
    "    \n",
    "### NOTE: path to the datasets! './data/your_file_name.csv'\n",
    "\n",
    "#### Warmup Data\n",
    " **_vectorize.csv_**: data used during the vectorization warmup for Assignment 4\n",
    "\n",
    "\n",
    "### Imports\n",
    "**NOTE:** We are only allowing four imports: numpy, math, collections.Counter and time. We will be checking to see if any other libraries are used. You are not allowed to use any outside libraries especially for part 4 (challenge). Please remember that you should not change any function headers other than in part 4.\n",
    "\n",
    "\n",
    "# HOW TO USE THIS NOTEBOOK\n",
    "\n",
    "## This notebook is meant to help structure your coding for the assignment, all it does is to align with the relevant tests for each section in a convenient format. Code Changes should still be made in 'submission.py' file. This notebook *should* dynamically reload your file for each test (please let us know if it doesn't). You do not need to submit this notebook, nor do you need to use it if you prefer running the unit tests from the command line. Remember to read the unit tests to understand what you are passing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf8fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting Up some utilities for testing:͏󠄂͏️͏󠄌͏󠄎͏︈͏͏󠄁\n",
    "from __future__ import division\n",
    "\n",
    "import unittest\n",
    "import submission as dt\n",
    "import numpy as np\n",
    "import importlib\n",
    "import decision_trees_submission_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03dc3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def single_tester(test, case):\n",
    "    importlib.reload(dt)\n",
    "    importlib.reload(decision_trees_submission_tests)\n",
    "    if test == decision_trees_submission_tests.DecisionTreePart1Tests:\n",
    "        print(\"Running Decision Tree Part 1 Test: {}\".format(case))\n",
    "    elif test == decision_trees_submission_tests.DecisionTreePart2Tests:\n",
    "        print(\"Running Decision Tree Part 2 Test: {}\".format(case))\n",
    "    elif test == decision_trees_submission_tests.DecisionTreePart3Tests:\n",
    "        print(\"Running Decision Tree Part 3 Test: {}\".format(case))\n",
    "    elif test == decision_trees_submission_tests.DecisionTreePart4Tests:\n",
    "        print(\"Running Decision Tree Part 4 Test: {}\".format(case))\n",
    "    elif test == decision_trees_submission_tests.VectorizationWarmUpTests:\n",
    "        print(\"Running Vectoriization Warmup Tests: {}\".format(case))\n",
    "    elif test == decision_trees_submission_tests.NameTests:\n",
    "        print(\"Name Test: {}\".format(case))\n",
    "        \n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTest(test(case))\n",
    "    runner = unittest.TextTestRunner()\n",
    "    runner.run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5184814d",
   "metadata": {},
   "source": [
    "### Part 0: Vectorization!\n",
    "_[10 pts]_\n",
    "\n",
    "* File to use: **_vectorize.csv_**\n",
    "\n",
    "The vectorization portion of this assignment will teach you how to use matrices to significantly increase the speed and decrease processing complexity of your AI problems. Matrix operations, linear algebra and vector space axioms and operations will be challenging to learn. Learn this, and it will benefit you throughout OMSCS courses and your career.\n",
    "\n",
    "You will not be able to meet time requirements, nor process large datasets without Vectorization Whether one is training a deep neural network on millions of images, building random forests for a U.S. National Insect Collection dataset, or optimizing algorithms, machine learning requires extensive use of vectorization. The NumPy Open Source project provides the numpy python scientific computing package using low-level optimizations written in C. If you find the libraries useful, consider contributing to the body of knowledge (e.g. help update and advance numpy)\n",
    "\n",
    "You will not meet the time limits on this assignment without vectorization. This small section will introduce you to vectorization and one of the high-demand technologies in python. We encourage you to use any numpy function to do the functions in the warmup section. You will need to beat the non-vectorized code to get full points.\n",
    "\n",
    "As a reminder, TAs will not help on this section. This section was created to help get you ready for this and other assignments; feel free to ask other students on Ed Discussion or use some training resources. (e.g. https://numpy.org/learn/).\n",
    "\n",
    "How grading works:\n",
    "1. We run the non-vectorized code and your vectorized code 500 times, as long as the average time of your vectorized code is less than the average time of the non-vectorized code, you will get the points (given that your answer is correct).\n",
    "\n",
    "#### Functions to complete in the `Vectorization` class:\n",
    "1. `vectorized_loops()`\n",
    "2. `vectorized_slice()`\n",
    "3. `vectorized_flatten()`\n",
    "4. `vectorized_glue()`\n",
    "5. `vectorized_mask()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a91e6",
   "metadata": {},
   "source": [
    "1. `vectorized_loops()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b18455",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_loops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_loops_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60224ec3",
   "metadata": {},
   "source": [
    "2. `vectorized_slice()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_slice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_slice_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc266839",
   "metadata": {},
   "source": [
    "3. `vectorized_flatten()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21029a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_flatten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b2ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_flatten_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec7baa",
   "metadata": {},
   "source": [
    "4. `vectorized_glue()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_glue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d030a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_glue_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a1bd5c",
   "metadata": {},
   "source": [
    "5. `vectorized_mask()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5594feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.VectorizationWarmUpTests, 'test_vectorized_mask_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02164744",
   "metadata": {},
   "source": [
    "## The Assignment\n",
    "Classification is used widely in machine learning to figure out how to sort new data that comes through.  You will build, train and test decision tree models to perform basic classification tasks. Students should understand how decision trees and random forests work. This will help you develop an intuition for how and why accuracy differs for training and testing data based on different parameters.\n",
    "\n",
    "## Introduction\n",
    "For this assignment we need an explicit way to make structured decisions. The DecisionNode class will be used to represent a decision node as some atomic choice in a multi-class decision graph. You must use this implementation for the nodes of the Decision Tree for this assignment to pass the tests and receive credit.\n",
    "\n",
    "An object of type 'DecisionNode' can represent a\n",
    "\n",
    "  * decision node\n",
    "     - left: will point to less than or equal values of the split value, type DecisionNode, True evaluations\n",
    "     - right: will point to greater than values of the split value, type DecisionNode, False evaluations\n",
    "     - decision_function: evaluates an attribute's value and maps each vector to a descendant\n",
    "     - class_label: None\n",
    "  * leaf node\n",
    "     - left: None\n",
    "     - right: None\n",
    "     - decision_function: None\n",
    "     - class_label: A leaf node's class value\n",
    "  * Note that in this representation 'True' values for a decision take us to the left.\n",
    "\n",
    "\n",
    "### Part 1a: Building a Tree by Hand\n",
    "_[10 Pts]_\n",
    "\n",
    "In `build_decision_tree()`, construct a tree of decision nodes by hand in order to classify the data below, i.e. map each datum **x** to a label **y**.  Select tests to be as small as possible (in terms of attributes), breaking ties among tests with the same number of attributes by selecting the one that classifies the greatest number of examples correctly. If multiple tests have the same number of attributes and classify the same number of examples, then break the tie using attributes with lower index numbers (e.g. select **A1** over **A2**)\n",
    "\n",
    "<p>\n",
    "<img src=\"./files/Decision_tree_hand.png\" alt=\"Decision Trees\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "#### Requirements:\n",
    "The total number of elements(nodes, leaves) in your tree should be < 10.\n",
    "\n",
    "#### Hints:\n",
    "To get started, it might help to **draw out the tree by hand** with each attribute representing a node.\n",
    "\n",
    "To create the decision function that will be passed to `DecisionNode`, you can create a lambda expression as follows:\n",
    "\n",
    "    func = lambda feature : feature[2] <= 0.356\n",
    "\n",
    "This will choose the left node if the third (A2) attribute is <= 0.356.\n",
    "\n",
    "For example, a tree looks like this:\n",
    "                                                                 \n",
    "    func = lambda feature : feature[0] <= -0.918                                                                 \n",
    "\n",
    "> in this example if feature[0] is evaluated as true then it would belong to the leaf for class = 1; else class = 0\n",
    "> <p>\n",
    "> <img src=\"./files/tree_example.png\" alt=\"Tree Example\"/>\n",
    "\n",
    "You would write your code like this:\n",
    "\n",
    "    func0 = lambda feature : feature[0] <= -0.918\n",
    "    decision_tree_root = DecisionNode(None, None, func0, None)\n",
    "    or\n",
    "    decision_tree_root = DecisionNode(None, None, lambda feature : feature[0] <= -0.918, None)\n",
    "    decision_tree_root.left = DecisionNode(None, None, None, class1)\n",
    "    decision_tree_root.right = DecisionNode(None, None, None, class0)\n",
    "    return decision_tree_root\n",
    "\n",
    "#### Functions to complete in the `submission` module:\n",
    "1. `build_decision_tree()`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart1Tests, 'test_hand_tree_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b21e7",
   "metadata": {},
   "source": [
    "\n",
    "### Part 1b: Precision, Recall, Accuracy and Confusion Matrix\n",
    "_[12 pts]_\n",
    "\n",
    "To build the next generation of Starner Zapper, we will need to keep the high levels of Precision, Recall, and Accuracy inculcated in the legacy products. So you must find a new way. In binary or boolean classification we find these metrics in terms of true positives, false positives, true negatives, and false negatives. So it should be simple right?\n",
    "\n",
    "Your confusion matrix should be K x K, K = number of classes. Actual labels (true labels) of the dataset will be represented by the rows, and the predicted labels form the columns. Notice that the correct classifier predictions form the diagonal of the matrix. True positives are samples where the prediction matches the true label, false positives are samples that were predicted positive, but are actually negative. False negatives are samples that were predicted negative, but were actually positive, whereas true negatives were predicted negative and are negative. It will be very helpful to use the numpy diag (or diagonal) function in this part of the assignment. You will have to consider carefully by class what the diagonal value tells you, what its row tells you, what its column tells you, and what is left?\n",
    "\n",
    "    * Either of 2 accuracy calculations: Of all the examples, what percentage did my clf predict correctly?\n",
    "       - Balanced Accuracy\n",
    "       - Balanced Weighted Accuracy (recommended for challenge participants)\n",
    "    * Precision: How often is my classifier right when it makes a positive prediction?\n",
    "    * Recall: How often does my classifier recognize positive examples? Fill out the methods to compute the confusion matrix, accuracy, precision and recall for your classifier output. classifier_output will be the labels that your classifier predicts, while the true_labels will be the true test labels. Helpful references:\n",
    "\n",
    "\n",
    "  * Wikipedia: (https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "  * Metrics for Multi-Class Classification: (https://arxiv.org/pdf/2008.05756.pdf)\n",
    "  * Performance Metrics for Activity Recognition Sec 5: (https://www.nist.gov/system/files/documents/el/isd/ks/Final_PerMIS_2006_Proceedings.pdf#page=143)\n",
    "\n",
    "If you want to calculate the example set above by hand, run the following code.\n",
    "\n",
    "    classifier_output = [decision_tree_root.decide(example) for example in examples]\n",
    "\n",
    "    p1_confusion_matrix = confusion_matrix(classifier_output, classes)\n",
    "    p1_accuracy = accuracy( classifier_output, classes )\n",
    "    p1_precision = precision(classifier_output, classes)\n",
    "    p1_recall = recall(classifier_output, classes)\n",
    "\n",
    "    print p1_confusion_matrix, p1_accuracy, p1_precision, p1_recall\n",
    "\n",
    "#### Functions to complete in the `submission` module:\n",
    "\n",
    "1. `confusion_matrix()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c09321",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart1Tests, 'test_confusion_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a85def",
   "metadata": {},
   "source": [
    "2. `precision()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc68fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart1Tests, 'test_precision_calculation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0249f3",
   "metadata": {},
   "source": [
    "3. `recall()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f860626",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart1Tests, 'test_recall_calculation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19104b7",
   "metadata": {},
   "source": [
    "4. `accuracy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e7e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart1Tests, 'test_accuracy_calculation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba82ccb",
   "metadata": {},
   "source": [
    "\n",
    "### Part 2a: Decision Tree Learning\n",
    "_[10 pts]_\n",
    "\n",
    "Purity, we strive for purity, alike Sir Galahad the Pure... Splitting at a decision is all about purity. You are trying to improve information gain which means, you are trying to gain purer divisions of the data. Through purer divisions of the data it is more ordered. This relates to entropy in physics, where ordered motion produces more energy. Through ordered data you gain more information on the defining characteristics (attributes) of something observed.\n",
    "\n",
    "We will use GINI impurity and the GINI Impurity Index to calculate the gini_impurity and gini_gain() on the splits to calculate Information Gain. The challenge will be to choose the best attribute at each decision with the lowest impurity and the highest index. At each attribute we search for the best value to split on, the hypotheses are compared against what we currently know, because would we want to split if we learn nothing? Hints:\n",
    "\n",
    " * Gini impurity (https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity)\n",
    " * Information gain (https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)\n",
    " * The Gini Gain follows a similar approach to information gain, replacing entropy with Gini Impurity.\n",
    " * Numpy helpful functions include advanced indexing, and filtering arrays with masks, slicing, stacking and concatenating\n",
    "\n",
    "<p>\n",
    "\n",
    "#### Functions to complete in the `submission` module:\n",
    "1. `gini_impurity()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb08b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_gini_impurity_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef6facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_gini_impurity_min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414928bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_gini_impurity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec9e8e",
   "metadata": {},
   "source": [
    "2. `gini_gain()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e686f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_gini_gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7a1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_gini_gain_max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b0e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_gini_gain_restaurant_patrons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_gini_gain_restaurant_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60547a65",
   "metadata": {},
   "source": [
    "### Part 2b: Decision Tree Learning\n",
    "_[25 pts]_\n",
    "\n",
    "\n",
    "   *  Data to train and test with: **_simple_binary.csv, simple_multi.csv, mod_complex_binary.csv, mod_complex_multi.csv_**\n",
    "   *  Grading:   \n",
    "      - 15 pts: average test accuracy over 10 rounds should be >= 50%\n",
    "      - 20 pts: average test accuracy over 10 rounds should be >= 60%\n",
    "      - 25 pts: average test accuracy over 10 rounds should be >= 75%\n",
    "\n",
    "Meanwhile in the lab... As the size of our flying training set grows, it rapidly becomes impractical to build multiclass trees by hand. We need to add a class with member functions to manage this, it is too much! \n",
    "To do list:\n",
    "      - Initialize the class with useful variables and assignments\n",
    "      - Fill out the member function that will fit the data to the tree, using build\n",
    "      - Fill out the build function\n",
    "      - Fill out the classify function\n",
    "\n",
    "For starters, consider these helpful hints for the construction of a decision tree from a given set of examples:\n",
    "\n",
    "   1. Watch your base cases:\n",
    "      - If all input vectors have the same class, return a leaf node with the appropriate class label.\n",
    "      - If a specified depth limit is reached, return a leaf labeled with the most frequent class.\n",
    "      - Splits producing 0, 1 length vectors\n",
    "      - Splits producing less or equivalent information\n",
    "      - Division by zero\n",
    "   2. Use the DecisionNode class\n",
    "   3. For each attribute alpha: evaluate the information gained by splitting on the attribute alpha.\n",
    "   4. Let alpha_best be the attribute value with the highest information gain.\n",
    "   5. As you progress in this assignment this is going to be tested against larger and more complex datasets, think about how it will affect your identification and selection of values to test.\n",
    "   6. Create a decision node that splits on alpha_best and split the data and classes by this value.\n",
    "   7. When splitting a dataset and classes, they must stay synchronized, do not orphan or shift the indexes independently\n",
    "   8. Use recursion to build your tree, by using the split lists, remember true goes left using decide\n",
    "   9. Your features and classify should be in numpy arrays where for dataset of size (m x n) the features would be (m x n-1) and classify would be (m x 1)\n",
    "   10. The features are real numbers, you will need to split based on a threshold. Consider different approaches for what this threshold might be.\n",
    "\n",
    "\n",
    "First, in the `DecisionTree.__build_tree__()` method implement the above algorithm.\n",
    "Next, in `DecisionTree.classify()`, write a function to produce classifications for a list of features once your decision tree has been built.\n",
    "\n",
    "Some other helpful notes:\n",
    "1. Your features and classify should be in numpy arrays where if the dataset is (_m_ x _n_) then the features is (_m_ x _n_-1) and classify is (_m_ x _1_)\n",
    "2. These features are continuous features and you will need to split based on a threshold.\n",
    "\n",
    "How grading works:\n",
    "1. We load **_mod_complex_multi.csv_** and create our cross-validation training and test set with a `k=10` folds.  We use our own `generate_k_folds()` method.\n",
    "2. We classify the (folder) training data onto the tree then fit the testing data onto the tree.\n",
    "3. We check the accuracy of your results versus the true results, and we return the average of this over 10 iterations.\n",
    "\n",
    "#### Functions to complete in the `DecisionTree` class:\n",
    "1. `__build_tree__()`\n",
    "2. `fit()`\n",
    "3. `classify()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426b6782",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_decision_tree_all_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bdf56f",
   "metadata": {},
   "source": [
    "### Part 2c: Validation\n",
    "_[7 pts]_\n",
    "\n",
    "* File to use: **_mod_complex_multi.csv_**\n",
    "* Allowed use of numpy, collections.Counter, and math.log\n",
    "* Grading: average test accuracy over 10 rounds should be >= 80%\n",
    "\n",
    "Reserving part of your data as a test set can lead to unpredictable performance as you may not have a complete set or proportionately balanced representation of the class labels. We can overcome this limitation by using k-fold cross validation. There are many benefits to using cross-validation. See book 3rd Ed Evaluating and Choosing the Best Hypothesis 18.4.\n",
    "\n",
    "In `generate_k_folds()`, we'll split the dataset at random into k equal subsections. Then iterating on each of our k samples, we'll reserve that sample for testing and use the other k-1 for training. Averaging the results of each fold should give us a more consistent idea of how the classifier is doing across the data as a whole.\n",
    "\n",
    "For those who are not familiar with k folds cross-validation, please refer the tutorial here: A Gentle Introduction to k-fold Cross-Validation (https://machinelearningmastery.com/k-fold-cross-validation/).\n",
    "\n",
    "How grading works:\n",
    "1. The same as 2b however, we use your `generate_k_folds()` instead of ours.\n",
    "\n",
    "#### Functions to complete in the `submission` module:\n",
    "1. `generate_k_folds()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb26399",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_k_folds_training_set_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart2Tests, 'test_k_folds_test_set_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2eb50e",
   "metadata": {},
   "source": [
    "### Part 3: Random Forests\n",
    "_[25 pts]_\n",
    "\n",
    "* File to use: **_mod_complex_binary.csv, mod_complex_multi.csv_**\n",
    "* Allowed use of numpy, collections.Counter, and math.log\n",
    "* Allowed to write additional functions to improve your score\n",
    "* Allowed to switch to Entropy and splitting entropy\n",
    "* Grading: \n",
    "\n",
    "    - 15 pts: average test accuracy over 10 rounds should be >= 50%\n",
    "    - 20 pts: average test accuracy over 10 rounds should be >= 70%\n",
    "    - 25 pts: average test accuracy over 10 rounds should be >= 80%\n",
    "\n",
    "The decision boundaries drawn by decision trees are very sharp, and fitting a decision tree of unbounded depth to a list of training examples almost inevitably leads to overfitting. In an attempt to decrease the variance of our classifier we're going to use a technique called 'Bootstrap Aggregating' (often abbreviated as 'bagging').\n",
    "\n",
    "A Random Forest is a collection of decision trees, built as follows:\n",
    "\n",
    "1. For every tree we're going to build:\n",
    "    - Subsample the examples provided us (with replacement) in accordance with a provided example subsampling rate.\n",
    "    - From the sample in the first step, choose attributes (without replacement) at random to learn on (in accordance with a provided attribute subsampling rate).\n",
    "    - Fit a decision tree to the subsample of data we've chosen (to a certain depth).\n",
    "\n",
    "Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n",
    "\n",
    "When sampling attributes you choose from the entire set of attributes for every sample drawn (but specify the sampling method does not use replacement)\n",
    "\n",
    "Fill in `RandomForest.fit()` to fit the decision tree as we describe above, and fill in `RandomForest.classify()` to classify a given list of examples.\n",
    "\n",
    "Your features and classify should be in numpy arrays where if the dataset is (_m_ x _n_) then the features is (_m_ x _n_-1) and classify is (_n_ x _1_).\n",
    "\n",
    "To test, we will be using a forest with 80 trees, with a depth limit of 5, example subsample rate of 0.3 and attribute subsample rate of 0.3\n",
    "\n",
    "How grading works:\n",
    "1. Similar to 2b but with the call to Random Forest.\n",
    "\n",
    "#### Functions to complete in the `RandomForest` class:\n",
    "1. `fit()`\n",
    "2. `classify()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516cde76",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart3Tests, 'test_binary_random_forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e91ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart3Tests, 'test_multi_random_forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a289fa6",
   "metadata": {},
   "source": [
    "### Part 4: Challenge Classifier\n",
    "_[Extra Credit]_\n",
    "\n",
    "#### This will be a separate submission.\n",
    "\n",
    "* File to use: **_ccomplex_binary.csv, complex_multi.csv_**\n",
    "* Allowed use of numpy, collections.Counter, and math.log\n",
    "* Allowed to write additional functions to improve your score\n",
    "* Allowed to switch to Entropy and splitting entropy\n",
    "* Ranked by Balanced Accuracy Weighted, Precision, and Recall.\n",
    "* Ties broken by efficiency (speed).\n",
    "* Extra Credit Points:\n",
    "   \n",
    "    - 5 pts: 1st place algorithm test over 10 rounds\n",
    "    - 4 pts: 2nd place algorithm test over 10 rounds\n",
    "    - 3 pts: 3rd place algorithm test over 10 rounds\n",
    "    - 2 pts: 4th place algorithm test over 10 rounds\n",
    "    - 1 pt: 5th place algorithm test over 10 rounds\n",
    "\n",
    "   \n",
    "Decision boundaries drawn by decision trees are very sharp, and fitting a decision tree of unbounded depth to a set of training examples almost inevitably leads to overfitting. In an attempt to decrease the variance of your classifier you are going to use a technique called 'Boosting' implementing one of the boosting algorithms such as, Ada-, Gradient- and XG-, boost or your personal favorite. Similar to RF, the Decision stumps are short decision trees used in these Ensemble classification methods\n",
    "\n",
    "    - They are usually short (depth limited)\n",
    "    - They use smaller (but more of them) random datasets for training with sampling bias\n",
    "    - They use a subset of attributes sampled from the training set\n",
    "    - They fit the tree to the sampled dataset and are considered specialized to the set\n",
    "    - They use weighting of their sampling and classifiers to reflect the balance or unbalance of the data\n",
    "    - They use majority voting (every tree in the forest votes) to classify a sample\n",
    "\n",
    "When sampling attributes you choose from the entire set of attributes without replacement based on the weighted distribution. Notice that you favor or bias towards misclassified samples, which improves your overall accuracy. Visualize how the short trees balance classification bias.\n",
    "\n",
    "Complete ChallengeClassifier.fit() to fit the decision tree as we describe above, and fill in ChallengeClassifier.classify() to classify examples. Use your decision tree implementation as your classifier or create another.\n",
    "\n",
    "Your features and classify should use numpy arrays datasets of (m x n) features of (m x n-1) and classes of (m x 1).\n",
    "\n",
    "* How grading works: \n",
    "    - To test, we will be running 10 rounds, using your boosting with 200 trees, with a depth limit of 3, example subsample rate of 0.1 and attribute subsample rate of 0.2. You will have a time limit.\n",
    "\n",
    "#### Functions to complete in the `ChallengeClassifier` class:\n",
    "\n",
    "1. `fit()`\n",
    "2. `boost()`\n",
    "3. `classify()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart4Tests, 'test_binary_boosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d8d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.DecisionTreePart4Tests, 'test_multi_boosting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c34fb41",
   "metadata": {},
   "source": [
    "### Part 5: Return Your name!\n",
    "_[1 pts]_\n",
    "Return your name from the function `return_your_name()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d0d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tester(decision_trees_submission_tests.NameTests, 'test_name')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
